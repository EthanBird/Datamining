{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "def normalize_text(text):\n",
    "    tm1 = re.sub('<pre>.*?</pre>', '', text, flags=re.DOTALL)\n",
    "    tm2 = re.sub('<code>.*?</code>', '', tm1, flags=re.DOTALL)\n",
    "    tm3 = re.sub('<[^>]+>©', '', tm1, flags=re.DOTALL)\n",
    "    return tm3.replace(\"\\n\", \"\")\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "whereby\n",
      "often\n",
      "‘ve\n",
      "become\n",
      "‘s\n",
      "doing\n",
      "would\n",
      "that\n",
      "was\n"
     ]
    }
   ],
   "source": [
    "# Stop words\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "punctuations = string.punctuation\n",
    "stopwords = STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = English()\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
    "bert_ckpt_dir = os.path.join(\"model/\", bert_model_name)\n",
    "bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, bert_ckpt_file):\n",
    "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n",
    "    bert_output = bert(input_ids)\n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"swish\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    logits = keras.layers.Dense(units=8, activation=\"softmax\")(logits)\n",
    "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "    model.build(input_shape=(None, max_seq_len))\n",
    "    load_stock_weights(bert, bert_ckpt_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 128, 768)\n",
      "Done loading 196 BERT weights from: model/uncased_L-12_H-768_A-12\\bert_model.ckpt into <bert.model.BertModelLayer object at 0x000002330B062C40> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    }
   ],
   "source": [
    "model = create_model(128, bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 6152      \n",
      "=================================================================\n",
      "Total params: 109,486,856\n",
      "Trainable params: 109,486,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "  optimizer=keras.optimizers.Adam(1e-5),\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------load the model-----------------\n"
     ]
    }
   ],
   "source": [
    "checkpoint_save_path = \"./checkpoint/wine.ckpt\"\n",
    "\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    print('-------------load the model-----------------')\n",
    "    model.load_weights(checkpoint_save_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cabernet Sauvignon', 'Syrah', 'Riesling', 'Chardonnay', 'Pinot Noir', 'Sauvignon Blanc', 'Bordeaux-style Red Blend', 'Red Blend']\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./train_df.csv')\n",
    "test_df = pd.read_csv('./test_df.csv')\n",
    "classes = train_df.intent.unique().tolist()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-11 03:05:35\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: This is taut and mineral, with hidden power. It's very closed at this stage. It's a wine with acidity and notes of green fruits and crisp apples, with just a touch of wood. This will age for many years \n",
      "intent: Chardonnay\n",
      "\n",
      "text: This fruit is delicious. The fruits of the known fruit, darkness and water are filled. That's all. Rich, open, soft and strong  \n",
      "intent: Bordeaux-style Red Blend\n",
      "\n",
      "text: The aroma weaves a brocade of peaches, butter, grass, peas and citrus, and the taste buds are closely related to texture. With balanced acidity and chalkiness, supporting the taste of lemon and green herbs, this is a complex and easy to eat product \n",
      "intent: Chardonnay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "  \"This is taut and mineral, with hidden power. It's very closed at this stage. It's a wine with acidity and notes of green fruits and crisp apples, with just a touch of wood. This will age for many years\",\n",
    "  \"This fruit is delicious. The fruits of the known fruit, darkness and water are filled. That's all. Rich, open, soft and strong \",\n",
    "  \"The aroma weaves a brocade of peaches, butter, grass, peas and citrus, and the taste buds are closely related to texture. With balanced acidity and chalkiness, supporting the taste of lemon and green herbs, this is a complex and easy to eat product\"\n",
    "]\n",
    "pred_tokens = map(tokenizer.tokenize, sentences)\n",
    "pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "pred_token_ids = map(lambda tids: tids +[0]*(128-len(tids)),pred_token_ids)\n",
    "pred_token_ids = np.array(list(pred_token_ids))\n",
    "predictions = model.predict(pred_token_ids).argmax(axis=-1)\n",
    "for text, label in zip(sentences, predictions):\n",
    "    print(\"text:\", text, \"\\nintent:\", classes[label])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDetectionData:\n",
    "    DATA_COLUMN = \"text\"\n",
    "    LABEL_COLUMN = \"intent\"\n",
    "    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = 0\n",
    "        self.classes = classes\n",
    "        train, test = map(lambda df: df.reindex(df[IntentDetectionData.DATA_COLUMN].str.len().sort_values().index), [train, test])\n",
    "        ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n",
    "        \n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            text, label = row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "            x.append(token_ids)\n",
    "            y.append(self.classes.index(label))\n",
    "        return np.array(x), np.array(y)\n",
    "    \n",
    "    def _pad(self, ids):\n",
    "        x = []\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "        return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24368it [00:14, 1695.36it/s]\n",
      "11976it [00:07, 1698.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 180\n"
     ]
    }
   ],
   "source": [
    "data = IntentDetectionData(train_df, test_df, tokenizer, classes, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data.train_x,data.train_y).to_csv('train_xy_vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xy_vec = pd.read_csv('test_xy_vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytest_y = test_xy_vec['128'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytest_x = test_xy_vec[[str(i) for i in range(128)]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "      Cabernet Sauvignon       0.73      0.80      0.76      1747\n",
      "                   Syrah       0.67      0.67      0.67       782\n",
      "                Riesling       0.92      0.88      0.90       927\n",
      "              Chardonnay       0.90      0.86      0.88      2190\n",
      "              Pinot Noir       0.85      0.81      0.83      2490\n",
      "         Sauvignon Blanc       0.72      0.85      0.78       949\n",
      "Bordeaux-style Red Blend       0.79      0.83      0.81      1279\n",
      "               Red Blend       0.85      0.77      0.81      1612\n",
      "\n",
      "                accuracy                           0.81     11976\n",
      "               macro avg       0.80      0.81      0.81     11976\n",
      "            weighted avg       0.82      0.81      0.82     11976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_pred = model.predict(mytest_x).argmax(axis=-1)\n",
    "print(classification_report(mytest_y, my_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of load\n"
     ]
    }
   ],
   "source": [
    "print('end of load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
